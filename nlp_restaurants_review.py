# -*- coding: utf-8 -*-
"""NLP_Restaurants_Review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ckecudTx1xkUjviio0SCrhPxfOVmmtX4
"""

# Importing the Dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

# list which stores all the preprocessed texts
corpus = []

# Importing the Datasets
# using delimeted as t because its a Tab Separated Value(TSV)
dataset = pd.read_csv("Restaurant_Reviews.tsv", delimiter = '\t')

def text_preprocessing():
  nltk.download('stopwords')
  for i in range(0,1000):  # 1000 is the no. of reviews
    # replace all alphabets(except A to Z and a to z) with whitespace 
    review = re.sub('[^A-Za-z]',' ',dataset['Review'][i])
    # convert to lowercase, so that no difference in uppercase and lowercase
    review = review.lower()
    # split the text into words using delimeter as whitespace
    review = review.split()
    # stopwords we have to use is of english language
    all_stopwords = stopwords.words('english')
    # from stopwords we will remove not, because if we remove not stopwords from sentence then it can change its meaning
    all_stopwords.remove('not')
    # PosrterStemmer() used to reduce the word to its root word
    ps = PorterStemmer()
    # apply stemming to the words if the word doen't present in all_stopwords
    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]
    # join all the words back into text
    review = " ".join(review)
    # add the review in list
    corpus.append(review)

""" CountVectorizer is a great tool provided by the scikit-learn library in Python. 
    It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text.
    This is helpful when we have multiple such texts, and we wish to convert each word in each text into vectors (for using in further text analysis)"""

def bag_of_words():
  from sklearn.feature_extraction.text import CountVectorizer
  cv = CountVectorizer(max_features = 1500)
  x = cv.fit_transform(corpus).toarray()
  y = dataset.iloc[:,-1].values
  return [x,y]

def train_model():
  # splitting the dataset into training and test dataset
  from sklearn.model_selection import train_test_split
  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)

  # training the model using DecisionTreeClassifier
  from sklearn.tree import DecisionTreeClassifier
  classifier = DecisionTreeClassifier(random_state=0, criterion = 'entropy')
  classifier.fit(x_train,y_train)
  return [classifier, x_train, x_test, y_train, y_test]

def test_results():
  # predicting the test results
  y_pred = classifier.predict(x_test)
  # printing the predicted value vs the actual value
  print("predicted value vs actual value")
  print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1)),1))
  print()

  # Print Confusion Matrix
  from sklearn.metrics import confusion_matrix, accuracy_score
  cm = confusion_matrix(y_test, y_pred)
  print("Confusion Matrix:\n",cm)
  print()
  accuracy = accuracy_score(y_test,y_pred)
  print("Accuracy of model is: ",accuracy)
  print()

if __name__ == '__main__':
  # For text preprocessing
  text_preprocessing()

  # getting the value of x and y after applying countvectorizer
  x,y = bag_of_words()

  # perform training on the model
  classifier, x_train, x_test, y_train, y_test = train_model()

  # print the test results
  test_results()